{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial demonstrates how to use TVB-Optim's Network Dynamics framework to implement **Excitation-Inhibition Balance (EIB) tuning** for whole-brain models, following the methodology introduced by [Schirner et al. (2023)](https://doi.org/10.1038/s41467-023-38626-y). The approach combines **Feedback Inhibition Control (FIC)** to locally maintain E-I balance with **EIB tuning** to globally optimize network connectivity patterns."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install dependencies if running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"Running in Google Colab - installing dependencies...\")\n",
    "    !pip install -q tvboptim\n",
    "    print(\"✓ Dependencies installed!\")\n",
    "except ImportError:\n",
    "    pass  # Not in Colab, assume dependencies are available"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a two-population Reduced Wong-Wang model with separate excitatory and inhibitory populations, and optimize the network coupling to match empirical functional connectivity from resting-state fMRI. This biologically-inspired learning algorithm demonstrates how neural network parameters can be tuned to achieve desired functional connectivity while maintaining physiologically realistic dynamics.\n",
    "\n",
    "**What this tutorial covers:**\n",
    "\n",
    "- Two-population neural mass model with explicit E-I dynamics\n",
    "- Dual coupling mechanism (long-range excitation and feedforward inhibition)\n",
    "- Feedback Inhibition Control (FIC) to maintain target excitatory activity\n",
    "- EIB tuning algorithm to match empirical functional connectivity\n",
    "- BOLD signal simulation and FC computation\n",
    "- Both iterative (Part 2) and gradient-based (Part 3) optimization approaches\n",
    "\n",
    "**Reference:** Schirner, M., Deco, G., & Ritter, P. (2023). Learning how network structure shapes decision-making for bio-inspired computing. *Nature Communications*, *14*(1), Article 1. https://doi.org/10.1038/s41467-023-38626-y"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set up environment\n",
    "import os\n",
    "import time\n",
    "cpu = True\n",
    "if cpu:\n",
    "    N = 8\n",
    "    os.environ['XLA_FLAGS'] = f'--xla_force_host_platform_device_count={N}'\n",
    "\n",
    "# Import all required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as path_effects\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import copy\n",
    "import optax\n",
    "from scipy import io\n",
    "import equinox as eqx\n",
    "\n",
    "# Jax enable x64\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# Import from tvboptim\n",
    "from tvboptim.types import Parameter, BoundedParameter\n",
    "from tvboptim.types.stateutils import show_parameters\n",
    "from tvboptim.utils import set_cache_path, cache\n",
    "from tvboptim.optim.optax import OptaxOptimizer\n",
    "from tvboptim.optim.callbacks import MultiCallback, DefaultPrintCallback, SavingLossCallback\n",
    "\n",
    "# Network dynamics imports\n",
    "from tvboptim.experimental.network_dynamics import Network, solve, prepare\n",
    "from tvboptim.experimental.network_dynamics.dynamics.tvb import ReducedWongWang\n",
    "from tvboptim.experimental.network_dynamics.coupling import LinearCoupling, FastLinearCoupling\n",
    "from tvboptim.experimental.network_dynamics.graph import DenseDelayGraph, DenseGraph\n",
    "from tvboptim.experimental.network_dynamics.solvers import Heun, BoundedSolver\n",
    "from tvboptim.experimental.network_dynamics.noise import AdditiveNoise\n",
    "from tvboptim.data import load_structural_connectivity, load_functional_connectivity\n",
    "\n",
    "# BOLD monitoring\n",
    "from tvboptim.observations.tvb_monitors.bold import Bold\n",
    "\n",
    "# Observation functions\n",
    "from tvboptim.observations.observation import compute_fc, fc_corr, rmse\n",
    "\n",
    "# Caching utilities\n",
    "from tvboptim.utils import set_cache_path, cache\n",
    "\n",
    "# Set cache path for tvboptim\n",
    "set_cache_path(\"./ei_tuning\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Structural Data and Target FC\n",
    "\n",
    "We load the Desikan-Killiany parcellation structural connectivity and empirical functional connectivity from resting-state fMRI data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# Load structural connectivity with region labels\n",
    "weights, lengths, region_labels = load_structural_connectivity(name=\"dk_average\")\n",
    "\n",
    "# Normalize weights to [0, 1] range\n",
    "weights = weights / jnp.max(weights)\n",
    "n_nodes = weights.shape[0]\n",
    "\n",
    "# Delays\n",
    "speed = 3.0\n",
    "delays = lengths / speed\n",
    "\n",
    "# Load empirical functional connectivity as optimization target\n",
    "fc_target = load_functional_connectivity(name=\"dk_average\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# Define consistent color palette derived from cividis\n",
    "import matplotlib.colors as mcolors\n",
    "cividis_cmap = plt.cm.cividis\n",
    "cividis_colors = cividis_cmap(np.linspace(0, 1, 256))\n",
    "accent_blue = cividis_cmap(0.3)  # Dark blue from cividis\n",
    "accent_gold = cividis_cmap(0.85)  # Gold/yellow from cividis\n",
    "accent_mid = cividis_cmap(0.6)   # Mid-tone\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(8.1, 4))\n",
    "\n",
    "# Structural weights - use cividis\n",
    "im1 = ax1.imshow(weights, cmap='cividis', vmin=0, vmax=1)\n",
    "ax1.set_title('Structural Weights')\n",
    "ax1.set_xlabel('Region')\n",
    "ax1.set_ylabel('Region')\n",
    "plt.colorbar(im1, ax=ax1, fraction=0.046)\n",
    "\n",
    "# Delays - use cividis\n",
    "im2 = ax2.imshow(delays, cmap='cividis')\n",
    "ax2.set_title('Transmission Delays (ms)')\n",
    "ax2.set_xlabel('Region')\n",
    "ax2.set_ylabel('Region')\n",
    "plt.colorbar(im2, ax=ax2, fraction=0.046, label='ms')\n",
    "\n",
    "# Target FC - use cividis\n",
    "im3 = ax3.imshow(fc_target, vmin=0, vmax=1.0, cmap='cividis')\n",
    "ax3.set_title('Target Functional Connectivity')\n",
    "ax3.set_xlabel('Region')\n",
    "ax3.set_ylabel('Region')\n",
    "plt.colorbar(im3, ax=ax3, label='Correlation', fraction=0.046)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definitions\n",
    "\n",
    "### Two-Population Reduced Wong-Wang Model\n",
    "\n",
    "This model extends the standard Reduced Wong-Wang with explicit excitatory (E) and inhibitory (I) populations, each with separate synaptic gating variables (S_e, S_i) and transfer functions. This enables independent control of E-I balance via the J_i parameter."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "from typing import Tuple\n",
    "from tvboptim.experimental.network_dynamics.dynamics.base import AbstractDynamics\n",
    "from tvboptim.experimental.network_dynamics.core.bunch import Bunch\n",
    "\n",
    "class ReducedWongWangEIB(AbstractDynamics):\n",
    "    \"\"\"Two-population Reduced Wong-Wang model with E-I balance support\"\"\"\n",
    "\n",
    "    STATE_NAMES = ('S_e', 'S_i')\n",
    "    INITIAL_STATE = (0.001, 0.001)\n",
    "    AUXILIARY_NAMES = ('H_e', 'H_i')\n",
    "\n",
    "    DEFAULT_PARAMS = Bunch(\n",
    "        # Excitatory population parameters\n",
    "        a_e=310.0,         # Input gain parameter\n",
    "        b_e=125.0,         # Input shift parameter [Hz]\n",
    "        d_e=0.160,         # Input scaling parameter [s]\n",
    "        gamma_e=0.641/1000,  # Kinetic parameter\n",
    "        tau_e=100.0,       # NMDA decay time constant [ms]\n",
    "        w_p=1.4,           # Excitatory recurrence weight\n",
    "        W_e=1.0,           # External input scaling weight\n",
    "\n",
    "        # Inhibitory population parameters\n",
    "        a_i=615.0,         # Input gain parameter\n",
    "        b_i=177.0,         # Input shift parameter [Hz]\n",
    "        d_i=0.087,         # Input scaling parameter [s]\n",
    "        gamma_i=1.0/1000,  # Kinetic parameter\n",
    "        tau_i=10.0,        # NMDA decay time constant [ms]\n",
    "        W_i=0.7,           # External input scaling weight\n",
    "\n",
    "        # Synaptic weights\n",
    "        J_N=0.15,          # NMDA current [nA]\n",
    "        J_i=1.0,           # Inhibitory synaptic weight\n",
    "\n",
    "        # External inputs\n",
    "        I_o=0.382,         # Background input current\n",
    "        I_ext=0.0,         # External stimulation current\n",
    "\n",
    "        # Coupling parameters\n",
    "        lamda=1.0,         # Lambda: inhibitory coupling scaling\n",
    "    )\n",
    "\n",
    "    COUPLING_INPUTS = {\n",
    "        'coupling': 2,  # Long-range excitation and Feedforward inhibition\n",
    "    }\n",
    "\n",
    "    def dynamics(\n",
    "        self,\n",
    "        t: float,\n",
    "        state: jnp.ndarray,\n",
    "        params: Bunch,\n",
    "        coupling: Bunch,\n",
    "        external: Bunch\n",
    "    ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "        \"\"\"Compute two-population Wong-Wang dynamics with dual coupling.\"\"\"\n",
    "\n",
    "        # Unpack state variables\n",
    "        S_e = state[0]  # Excitatory synaptic gating\n",
    "        S_i = state[1]  # Inhibitory synaptic gating\n",
    "\n",
    "        # Unpack coupling inputs\n",
    "        c_lre = params.J_N * coupling.coupling[0]  # Long-range excitation\n",
    "        c_ffi = params.J_N * coupling.coupling[1]  # Feedforward inhibition\n",
    "\n",
    "        # Excitatory population input\n",
    "        J_N_S_e = params.J_N * S_e\n",
    "        x_e_pre = (params.w_p * J_N_S_e - params.J_i * S_i +\n",
    "                   params.W_e * params.I_o + c_lre + params.I_ext)\n",
    "\n",
    "        # Excitatory transfer function\n",
    "        x_e = params.a_e * x_e_pre - params.b_e\n",
    "        H_e = x_e / (1.0 - jnp.exp(-params.d_e * x_e))\n",
    "\n",
    "        # Excitatory dynamics\n",
    "        dS_e_dt = -(S_e / params.tau_e) + (1.0 - S_e) * H_e * params.gamma_e\n",
    "\n",
    "        # Inhibitory population input\n",
    "        x_i_pre = J_N_S_e - S_i + params.W_i * params.I_o + params.lamda * c_ffi\n",
    "\n",
    "        # Inhibitory transfer function\n",
    "        x_i = params.a_i * x_i_pre - params.b_i\n",
    "        H_i = x_i / (1.0 - jnp.exp(-params.d_i * x_i))\n",
    "\n",
    "        # Inhibitory dynamics\n",
    "        dS_i_dt = -(S_i / params.tau_i) + H_i * params.gamma_i\n",
    "\n",
    "        # Package results\n",
    "        derivatives = jnp.array([dS_e_dt, dS_i_dt])\n",
    "        auxiliaries = jnp.array([H_e, H_i])\n",
    "\n",
    "        return derivatives, auxiliaries"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual-Weight EIB Coupling\n",
    "\n",
    "This coupling mechanism produces two outputs from incoming excitatory activity: long-range excitation (wLRE) and feedforward inhibition (wFFI). Separate weight matrices enable independent tuning of excitatory and inhibitory pathways."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "from tvboptim.experimental.network_dynamics.coupling.base import InstantaneousCoupling\n",
    "\n",
    "class EIBLinearCoupling(InstantaneousCoupling):\n",
    "    \"\"\"EIB Linear coupling with separate excitatory and inhibitory weight matrices.\n",
    "\n",
    "    This coupling produces two outputs:\n",
    "        c_lre: Long-range excitation (wLRE * S_e)\n",
    "        c_ffi: Feedforward inhibition (wFFI * S_e)\n",
    "\n",
    "    Both couplings are driven by the excitatory activity (S_e) from other regions.\n",
    "    \"\"\"\n",
    "\n",
    "    N_OUTPUT_STATES = 2  # Produces two coupling outputs\n",
    "\n",
    "    DEFAULT_PARAMS = Bunch(\n",
    "        wLRE = 1.0,  # Long-range excitation weight matrix\n",
    "        wFFI = 1.0,  # Feedforward inhibition weight matrix\n",
    "    )\n",
    "\n",
    "    def pre(\n",
    "        self,\n",
    "        incoming_states: jnp.ndarray,\n",
    "        local_states: jnp.ndarray,\n",
    "        params: Bunch\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Pre-synaptic transformation: multiply S_e with wLRE and wFFI.\"\"\"\n",
    "        # incoming_states[0] is S_e from all source nodes\n",
    "        S_e = incoming_states[0]  # [n_target, n_source]\n",
    "        # Apply weights: element-wise multiply S_e with each weight matrix\n",
    "        # params.wLRE and params.wFFI have shape [n_nodes, n_nodes]\n",
    "        c_lre = S_e * params.wLRE  # [n_target, n_source]\n",
    "        c_ffi = S_e * params.wFFI  # [n_target, n_source]\n",
    "\n",
    "        # Stack into [2, n_target, n_source]\n",
    "        return jnp.stack([c_lre, c_ffi], axis=0)\n",
    "\n",
    "    def post(\n",
    "        self,\n",
    "        summed_inputs: jnp.ndarray,\n",
    "        local_states: jnp.ndarray,\n",
    "        params: Bunch\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Post-synaptic transformation: pass through without scaling.\"\"\"\n",
    "        return summed_inputs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Network Model\n",
    "\n",
    "We combine the EIB dynamics with structural connectivity and initialize the dual weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# Create network components\n",
    "graph = DenseGraph(weights, region_labels=region_labels)\n",
    "dynamics = ReducedWongWangEIB(J_i = jnp.ones((n_nodes)))\n",
    "\n",
    "# Initialize EIB coupling with dual weight matrices\n",
    "# wLRE and wFFI start as copies of structural connectivity\n",
    "coupling = EIBLinearCoupling(incoming_states=[\"S_e\"])\n",
    "\n",
    "# Set the weight matrices to the proper shape based on structural connectivity\n",
    "# Both start as scaled versions of structural connectivity\n",
    "coupling.params.wLRE = jnp.ones((n_nodes, n_nodes)) #+ 0.8*fc_target  # [n_nodes, n_nodes]\n",
    "coupling.params.wFFI = jnp.ones((n_nodes, n_nodes)) #- 0.8*fc_target  # [n_nodes, n_nodes]\n",
    "\n",
    "# Small noise to break symmetry\n",
    "noise = AdditiveNoise(sigma=0.01, apply_to=\"S_e\")\n",
    "\n",
    "# Assemble the network\n",
    "network = Network(\n",
    "    dynamics=dynamics,\n",
    "    coupling={'coupling': coupling},  # Both use same coupling but produce different outputs\n",
    "    graph=graph,\n",
    "    noise=noise\n",
    ")\n",
    "\n",
    "print(f\"Network created with {n_nodes} nodes\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Simulation\n",
    "\n",
    "Before applying any tuning algorithms, we run an initial transient simulation to establish a baseline quasi-stationary state."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Prepare simulation: compile model and initialize state\n",
    "t1 = 5 * 60_000   # Simulation duration (ms) - 1 minute for initial transient\n",
    "dt = 4.0      # Integration timestep (ms) matching original script\n",
    "solver = BoundedSolver(Heun(), low=0.0, high=1.0)\n",
    "model, state = prepare(network, solver, t1=t1, dt=dt)\n",
    "\n",
    "# Run initial transient to reach quasi-stationary state\n",
    "print(\"Running initial transient simulation...\")\n",
    "result_init = jax.block_until_ready(model(state))\n",
    "\n",
    "# Update network with final state as new initial conditions\n",
    "network.update_history(result_init)\n",
    "\n",
    "# Prepare for shorter simulations used in EI tuning\n",
    "bold_TR = 720.0\n",
    "model_short, state_short = prepare(network, solver, t1=bold_TR, dt=dt)\n",
    "\n",
    "print(f\"Initial simulation complete. Final S_e mean: {result_init.data[-1, 0, :].mean():.3f}\")\n",
    "print(f\"Initial simulation complete. Final S_i mean: {result_init.data[-1, 1, :].mean():.3f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOLD Signal Setup\n",
    "\n",
    "We configure BOLD monitoring to convert neural activity into hemodynamic signals for FC computation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create BOLD monitor - we'll monitor S_e (first state variable)\n",
    "# The BOLD period is 720ms (TR) as in the original script\n",
    "bold_monitor = Bold(\n",
    "    period=bold_TR,           # BOLD sampling period (TR = 720 ms)\n",
    "    downsample_period=4.0,  # Intermediate downsampling matches dt\n",
    "    voi=0,                  # Monitor first state variable (S_e)\n",
    "    history=result_init     # Use initial state as warm start for BOLD history\n",
    ")\n",
    "\n",
    "print(\"BOLD monitor initialized\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "We define shared evaluation functions used throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# Will be populated after initial simulation completes\n",
    "model_eval, state_eval, _state = None, None, None\n",
    "\n",
    "def setup_eval_model():\n",
    "    \"\"\"Setup evaluation model for FC computation (called after initial simulation).\"\"\"\n",
    "    global model_eval, state_eval, _state\n",
    "    model_eval, state_eval = prepare(network, Heun(), t1=t1, dt=dt)\n",
    "    _state = copy.deepcopy(state_eval)\n",
    "\n",
    "def eval_fc(J_i, wLRE, wFFI):\n",
    "    \"\"\"Evaluate FC for given parameters using a long simulation.\"\"\"\n",
    "    _state.dynamics.J_i = J_i\n",
    "    _state.coupling.coupling.wLRE = wLRE\n",
    "    _state.coupling.coupling.wFFI = wFFI\n",
    "\n",
    "    # Run simulation\n",
    "    raw_result = model_eval(_state)\n",
    "\n",
    "    # Compute BOLD\n",
    "    bold_signal = bold_monitor(raw_result)\n",
    "\n",
    "    # Compute FC (skip initial transient)\n",
    "    fc = compute_fc(bold_signal, skip_t=20)\n",
    "    return fc\n",
    "\n",
    "print(\"Utility functions defined\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Feedback Inhibition Control (FIC)\n",
    "\n",
    "Neural mass models with explicit E-I populations face stability challenges: runaway excitation, complete silencing, or heterogeneous operating points across regions. FIC solves this by adaptively adjusting the local inhibitory weight **J_i** to maintain excitatory activity at a target level (~0.25).\n",
    "\n",
    "The update rule measures mean activity and adjusts inhibition proportionally:\n",
    "\n",
    "$$\\Delta J_i = \\eta_{FIC} \\cdot (\\langle S_i \\rangle \\langle S_e \\rangle - r_{target} \\langle S_i \\rangle)$$\n",
    "\n",
    "When $\\langle S_e \\rangle > r_{target}$, inhibition increases; when $\\langle S_e \\rangle < r_{target}$, it decreases."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "def FIC_update_rule(J_i, raw_data, eta_fic=0.1, target_fic=0.25):\n",
    "    \"\"\"Update J_i using FIC algorithm to maintain E-I balance.\"\"\"\n",
    "    # Compute mean activity over the simulation window\n",
    "    mean_S_i = jnp.mean(raw_data[:, 1], axis=0)  # Mean S_i over time [n_nodes]\n",
    "    mean_S_e = jnp.mean(raw_data[:, 0], axis=0)  # Mean S_e over time [n_nodes]\n",
    "\n",
    "    # FIC update rule: increase J_i if E activity is too high\n",
    "    # When mean_S_e > target_fic, d_J_i is positive, increasing inhibition\n",
    "    d_J_i = eta_fic * (mean_S_i * mean_S_e - target_fic * mean_S_i)\n",
    "    J_i_new = J_i + d_J_i\n",
    "\n",
    "    return J_i_new\n",
    "\n",
    "print(\"FIC update function defined\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running FIC Tuning\n",
    "\n",
    "Let's now apply FIC in a simple loop to see how it stabilizes the network dynamics. We'll run the simulation for multiple iterations, applying the FIC update rule after each step to gradually adjust J_i until excitatory activity converges to the target level."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# FIC tuning parameters\n",
    "eta_fic = 0.5  # Learning rate for FIC\n",
    "target_fic = 0.25  # Target excitatory activity level\n",
    "n_fic_steps = 200  # Number of FIC iterations\n",
    "\n",
    "@cache(\"fic_tuning\", redo=False)\n",
    "def run_fic_tuning():\n",
    "    \"\"\"Run FIC tuning loop with caching.\"\"\"\n",
    "    # Create a copy of the short simulation state for FIC tuning\n",
    "    state_fic = copy.deepcopy(state_short)\n",
    "    bold_monitor_fic = copy.deepcopy(bold_monitor)\n",
    "\n",
    "    # Store initial state for comparison\n",
    "    raw_result_pre_fic = model_short(state_fic)\n",
    "\n",
    "    # Setup for tracking BOLD signal during FIC\n",
    "    history_accessor = lambda tree: tree.history\n",
    "    bold_signal_fic = []\n",
    "    mean_S_e_history = []\n",
    "\n",
    "    # Random key for noise updates\n",
    "    key = jax.random.key(42)\n",
    "\n",
    "    print(\"Starting FIC tuning...\")\n",
    "\n",
    "    # FIC tuning loop\n",
    "    for i in range(n_fic_steps):\n",
    "        # Simulate one time step\n",
    "        raw_result = model_short(state_fic)\n",
    "\n",
    "        # Compute BOLD signal for this step\n",
    "        bold_result = bold_monitor_fic(raw_result)\n",
    "        bold_signal_fic.append(bold_result.ys[0, 0, :])\n",
    "\n",
    "        # Track mean excitatory activity\n",
    "        mean_S_e = jnp.mean(raw_result.data[:, 0, :])\n",
    "        mean_S_e_history.append(mean_S_e)\n",
    "\n",
    "        # Update BOLD monitor history for next iteration\n",
    "        new_history = jnp.roll(bold_monitor_fic.history, -raw_result.data.shape[0], axis=0)\n",
    "        new_history = new_history.at[-raw_result.data.shape[0]:, :, :].set(raw_result.data[:, 0:1, :])\n",
    "        bold_monitor_fic = eqx.tree_at(history_accessor, bold_monitor_fic, new_history)\n",
    "\n",
    "        # Update initial conditions for next iteration\n",
    "        state_fic.initial_state.dynamics = raw_result.data[-1]\n",
    "\n",
    "        # Update noise realization\n",
    "        key, subkey = jax.random.split(key, 2)\n",
    "        state_fic._internal.noise_samples = jax.random.normal(key=subkey, shape=state_fic._internal.noise_samples.shape)\n",
    "\n",
    "        # Apply FIC update rule\n",
    "        state_fic.dynamics.J_i = FIC_update_rule(state_fic.dynamics.J_i, raw_result.data, eta_fic=eta_fic, target_fic=target_fic)\n",
    "\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"  Step {i+1}/{n_fic_steps}, Mean S_e: {mean_S_e:.4f}, Target: {target_fic:.4f}\")\n",
    "\n",
    "    # Final simulation after FIC\n",
    "    raw_result_post_fic = model_short(state_fic)\n",
    "\n",
    "    # Convert lists to arrays\n",
    "    bold_signal_fic = jnp.array(bold_signal_fic)\n",
    "    mean_S_e_history = jnp.array(mean_S_e_history)\n",
    "\n",
    "    print(f\"FIC tuning complete!\")\n",
    "    print(f\"Final mean S_e: {mean_S_e_history[-1]:.4f} (target: {target_fic:.4f})\")\n",
    "\n",
    "    return {\n",
    "        'state_fic': state_fic,\n",
    "        'bold_monitor_fic': bold_monitor_fic,\n",
    "        'raw_result_pre_fic': raw_result_pre_fic,\n",
    "        'raw_result_post_fic': raw_result_post_fic,\n",
    "        'bold_signal_fic': bold_signal_fic,\n",
    "        'mean_S_e_history': mean_S_e_history\n",
    "    }\n",
    "\n",
    "# Run FIC tuning (cached)\n",
    "fic_results = run_fic_tuning()\n",
    "state_fic = fic_results['state_fic']\n",
    "bold_monitor_fic = fic_results['bold_monitor_fic']\n",
    "raw_result_pre_fic = fic_results['raw_result_pre_fic']\n",
    "raw_result_post_fic = fic_results['raw_result_post_fic']\n",
    "bold_signal_fic = fic_results['bold_signal_fic']\n",
    "mean_S_e_history = fic_results['mean_S_e_history']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIC Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "fig = plt.figure(figsize=(8.1, 7))\n",
    "gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Use cividis-derived colors for consistency\n",
    "target_color = accent_gold\n",
    "trace_color = accent_blue\n",
    "convergence_color = accent_mid\n",
    "\n",
    "# Top left: Pre-FIC timeseries\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.plot(raw_result_pre_fic.data[:, 0, :], alpha=0.6, linewidth=0.8, color=trace_color)\n",
    "ax1.axhline(target_fic, color=target_color, linestyle='--', linewidth=2, label=f'Target ({target_fic})')\n",
    "ax1.set_xlabel('Time step')\n",
    "ax1.set_ylabel('S_e (Excitatory activity)')\n",
    "ax1.set_title('Before FIC')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Top right: Post-FIC timeseries\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.plot(raw_result_post_fic.data[:, 0, :], alpha=0.6, linewidth=0.8, color=trace_color)\n",
    "ax2.axhline(target_fic, color=target_color, linestyle='--', linewidth=2, label=f'Target ({target_fic})')\n",
    "ax2.set_xlabel('Time step')\n",
    "ax2.set_ylabel('S_e (Excitatory activity)')\n",
    "ax2.set_title('After FIC')\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom left: Convergence\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "ax3.plot(mean_S_e_history, linewidth=2, label='Mean S_e', color=accent_blue)\n",
    "ax3.axhline(target_fic, color=target_color, linestyle='--', linewidth=2, label=f'Target ({target_fic})')\n",
    "ax3.set_xlabel('FIC iteration')\n",
    "ax3.set_ylabel('Mean S_e')\n",
    "ax3.set_title('FIC Convergence')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom right: BOLD signal evolution with mean overlay\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "# Plot all regions with light colors from cividis\n",
    "n_regions = bold_signal_fic.shape[1]\n",
    "colors_bold = cividis_cmap(np.linspace(0.2, 0.9, n_regions))\n",
    "for i in range(n_regions):\n",
    "    ax4.plot(bold_signal_fic[:, i], alpha=0.3, linewidth=0.8, color=colors_bold[i])\n",
    "# Overlay mean BOLD signal in darker color\n",
    "mean_bold = np.mean(bold_signal_fic, axis=1)\n",
    "ax4.plot(mean_bold, color=accent_blue, linewidth=2.5, label='Mean', alpha=0.9)\n",
    "ax4.set_xlabel('BOLD time point (TR)')\n",
    "ax4.set_ylabel('BOLD signal')\n",
    "ax4.set_title('BOLD Signal Evolution (all regions + mean)')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Excitation-Inhibition Balance (EIB) Tuning\n",
    "\n",
    "FIC establishes local E-I balance but doesn't control network-level functional connectivity. EIB extends this by adjusting the dual coupling weights (wLRE, wFFI) to match empirical FC patterns.\n",
    "\n",
    "The update rules increase wLRE and decrease wFFI when FC is too low, and vice versa when FC is too high:\n",
    "\n",
    "$$\\Delta w_{LRE}^{ij} = \\eta_{EIB} \\cdot (FC_{target}^{ij} - FC_{pred}^{ij}) \\cdot RMSE_i$$\n",
    "\n",
    "$$\\Delta w_{FFI}^{ij} = -\\eta_{EIB} \\cdot (FC_{target}^{ij} - FC_{pred}^{ij}) \\cdot RMSE_i$$\n",
    "\n",
    "The row-wise RMSE term weights updates by each region's overall FC error."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "def EI_update_rule(wLRE, wFFI, fc_pred, fc_target, eta_eib=0.02):\n",
    "    \"\"\"Update wLRE and wFFI using EIB algorithm\"\"\"\n",
    "    # Compute FC difference (positive means FC is too low, need more coupling)\n",
    "    diff_FC = fc_target - fc_pred\n",
    "\n",
    "    # Compute row-wise RMSE to weight updates by overall error magnitude\n",
    "    rmse_FC = rmse(fc_target, fc_pred, axis=1)[:, None]  # [n_nodes, 1]\n",
    "\n",
    "    # Update rules:\n",
    "    # - Increase wLRE when FC is too low (strengthen excitation)\n",
    "    # - Decrease wFFI when FC is too low (reduce inhibition)\n",
    "    # Note: opposite signs ensure coordinated adjustment\n",
    "    wLRE_new = jnp.clip(wLRE + eta_eib * diff_FC * rmse_FC, 0, None)\n",
    "    wFFI_new = jnp.clip(wFFI - eta_eib * diff_FC * rmse_FC, 0, None)\n",
    "\n",
    "    return wLRE_new, wFFI_new\n",
    "\n",
    "print(\"EIB update function defined\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined FIC+EIB Tuning\n",
    "\n",
    "We now run both algorithms simultaneously: FIC maintains local balance at each iteration, while EIB adjusts coupling weights based on a sliding window of BOLD signal to match target FC."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# Combined FIC+EIB tuning parameters\n",
    "eta_fic = 0.1  # FIC learning rate\n",
    "eta_eib = 0.005  # EIB learning rate (smaller than FIC)\n",
    "window_size = 150  # Number of BOLD TRs for FC calculation\n",
    "n_eib_steps = 2000  # Total number of iterations\n",
    "snapshot_interval = 50  # Collect snapshots every N iterations\n",
    "\n",
    "@cache(\"eib_tuning\", redo=False)\n",
    "def run_eib_tuning():\n",
    "    \"\"\"Run combined FIC+EIB tuning loop with caching.\"\"\"\n",
    "    # Initialize state for combined tuning\n",
    "    state_ei = copy.deepcopy(state_fic)\n",
    "    bold_monitor_ei = copy.deepcopy(bold_monitor_fic)\n",
    "    history_accessor = lambda tree: tree.history\n",
    "\n",
    "    # BOLD signal sliding window\n",
    "    bold_signal = bold_signal_fic[-window_size:].reshape((window_size, 1, n_nodes))\n",
    "\n",
    "    # Track metrics during tuning\n",
    "    fc_correlations = []\n",
    "    fc_rmse_values = []\n",
    "\n",
    "    # Random key for noise\n",
    "    key = jax.random.key(43)\n",
    "\n",
    "    # Store initial state for comparison\n",
    "    raw_result_pre_eib = model_short(state_ei)\n",
    "\n",
    "    # Data collection for animation\n",
    "    snapshots = {\n",
    "        'iterations': [],\n",
    "        'bold_signal': [],\n",
    "        'raw_timeseries': [],\n",
    "        'J_i': [],\n",
    "        'fc_pred': [],\n",
    "        'fc_corr': [],\n",
    "        'fc_rmse': [],\n",
    "        'wLRE': [],\n",
    "        'wFFI': [],\n",
    "    }\n",
    "\n",
    "    print(\"Starting combined FIC+EIB tuning...\")\n",
    "\n",
    "    # Combined FIC+EIB tuning loop\n",
    "    for i in range(n_eib_steps):\n",
    "        # 1. Simulate neural dynamics for one BOLD period\n",
    "        raw_result = model_short(state_ei)\n",
    "\n",
    "        # 2. Compute BOLD signal\n",
    "        bold_result = bold_monitor_ei(raw_result)\n",
    "\n",
    "        # 3. Update BOLD signal sliding window (rolling buffer)\n",
    "        bold_signal = jnp.roll(bold_signal, -1, axis=0)\n",
    "        bold_signal = bold_signal.at[-1, 0, :].set(bold_result.ys[0, 0, :])\n",
    "\n",
    "        # 4. Update BOLD monitor history for hemodynamic state continuity\n",
    "        new_history = jnp.roll(bold_monitor_ei.history, -raw_result.data.shape[0], axis=0)\n",
    "        new_history = new_history.at[-raw_result.data.shape[0]:, :, :].set(raw_result.data[:, 0:1, :])\n",
    "        bold_monitor_ei = eqx.tree_at(history_accessor, bold_monitor_ei, new_history)\n",
    "\n",
    "        # 5. Update initial conditions for next simulation\n",
    "        state_ei.initial_state.dynamics = raw_result.data[-1]\n",
    "\n",
    "        # 6. Update noise realization\n",
    "        key, subkey = jax.random.split(key, 2)\n",
    "        state_ei._internal.noise_samples = jax.random.normal(key=subkey, shape=state_ei._internal.noise_samples.shape)\n",
    "\n",
    "        # 7. Apply FIC update (every iteration)\n",
    "        state_ei.dynamics.J_i = FIC_update_rule(\n",
    "            state_ei.dynamics.J_i,\n",
    "            raw_result.data,\n",
    "            eta_fic=eta_fic,\n",
    "            target_fic=target_fic\n",
    "        )\n",
    "\n",
    "        # 8. Apply EIB update\n",
    "        # Compute FC from BOLD signal window\n",
    "        fc_pred = compute_fc(bold_signal)\n",
    "\n",
    "        # Update wLRE and wFFI using EIB rule\n",
    "        wLRE_new, wFFI_new = EI_update_rule(\n",
    "            state_ei.coupling.coupling.wLRE,\n",
    "            state_ei.coupling.coupling.wFFI,\n",
    "            fc_pred,\n",
    "            fc_target,\n",
    "            eta_eib=((i+1)/n_eib_steps) * eta_eib\n",
    "        )\n",
    "        state_ei.coupling.coupling.wLRE = wLRE_new\n",
    "        state_ei.coupling.coupling.wFFI = wFFI_new\n",
    "\n",
    "        # Track FC quality metrics\n",
    "        fc_corr_val = fc_corr(fc_pred, fc_target)\n",
    "        fc_rmse_val = jnp.sqrt(jnp.mean((fc_pred - fc_target)**2))\n",
    "        fc_correlations.append(fc_corr_val)\n",
    "        fc_rmse_values.append(fc_rmse_val)\n",
    "\n",
    "        # Collect snapshots for animation every N iterations\n",
    "        if (i + 1) % snapshot_interval == 0:\n",
    "            snapshots['iterations'].append(i + 1)\n",
    "            snapshots['bold_signal'].append(np.array(bold_signal[:, 0, :]))  # [window_size, n_nodes]\n",
    "            snapshots['raw_timeseries'].append(np.array(raw_result.data[:, 0, :]))  # [time_steps, n_nodes]\n",
    "            snapshots['J_i'].append(np.array(state_ei.dynamics.J_i.flatten()))  # [n_nodes]\n",
    "            snapshots['wLRE'].append(np.array(state_ei.coupling.coupling.wLRE))  # [n_nodes, n_nodes]\n",
    "            snapshots['wFFI'].append(np.array(state_ei.coupling.coupling.wFFI))  # [n_nodes, n_nodes]\n",
    "            snapshots['fc_pred'].append(np.array(fc_pred))\n",
    "            snapshots['fc_corr'].append(float(fc_corr_val))\n",
    "            snapshots['fc_rmse'].append(float(fc_rmse_val))\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"  Step {i+1}/{n_eib_steps}, FC corr: {fc_corr_val:.4f}, FC RMSE: {fc_rmse_val:.4f}\")\n",
    "\n",
    "    # Store final state for comparison\n",
    "    raw_result_post_eib = model_short(state_ei)\n",
    "\n",
    "    # Convert metrics to arrays\n",
    "    fc_correlations = jnp.array(fc_correlations)\n",
    "    fc_rmse_values = jnp.array(fc_rmse_values)\n",
    "\n",
    "    print(\"Combined FIC+EIB tuning complete!\")\n",
    "    print(f\"Collected {len(snapshots['iterations'])} snapshots for animation\")\n",
    "\n",
    "    return {\n",
    "        'state_ei': state_ei,\n",
    "        'raw_result_pre_eib': raw_result_pre_eib,\n",
    "        'raw_result_post_eib': raw_result_post_eib,\n",
    "        'fc_correlations': fc_correlations,\n",
    "        'fc_rmse_values': fc_rmse_values,\n",
    "        'snapshots': snapshots\n",
    "    }\n",
    "\n",
    "# Run EIB tuning (cached)\n",
    "eib_results = run_eib_tuning()\n",
    "state_ei = eib_results['state_ei']\n",
    "raw_result_pre_eib = eib_results['raw_result_pre_eib']\n",
    "raw_result_post_eib = eib_results['raw_result_post_eib']\n",
    "fc_correlations = eib_results['fc_correlations']\n",
    "fc_rmse_values = eib_results['fc_rmse_values']\n",
    "snapshots = eib_results['snapshots']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Tuning Progress: Animated GIF\n",
    "\n",
    "Now we can create an animated GIF showing the tuning progression using the collected snapshots."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "\n",
    "def create_animation_gif(output_path='ei_tuning_animation.gif', fps=2):\n",
    "    \"\"\"Create an animated GIF showing the tuning progression.\"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8), layout = \"tight\")\n",
    "\n",
    "    def update_frame(snapshot_idx):\n",
    "        \"\"\"Update function for animation.\"\"\"\n",
    "        fig.clear()\n",
    "        gs = fig.add_gridspec(3, 3, hspace=0.4, wspace=0.5)\n",
    "\n",
    "        iteration = snapshots['iterations'][snapshot_idx]\n",
    "        bold_sig = snapshots['bold_signal'][snapshot_idx]\n",
    "        raw_ts = snapshots['raw_timeseries'][snapshot_idx]\n",
    "        J_i_vals = snapshots['J_i'][snapshot_idx]\n",
    "        wLRE_mat = snapshots['wLRE'][snapshot_idx]\n",
    "        wFFI_mat = snapshots['wFFI'][snapshot_idx]\n",
    "        fc_pred_mat = snapshots['fc_pred'][snapshot_idx]\n",
    "        fc_corr_val = snapshots['fc_corr'][snapshot_idx]\n",
    "        fc_rmse_val = snapshots['fc_rmse'][snapshot_idx]\n",
    "\n",
    "        # Define harmonized colors for animation\n",
    "        cividis_cmap = plt.cm.cividis\n",
    "        anim_blue = cividis_cmap(0.3)\n",
    "        anim_gold = cividis_cmap(0.85)\n",
    "        anim_mid = cividis_cmap(0.6)\n",
    "\n",
    "        # Row 1: BOLD signal and raw timeseries\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        # Plot all BOLD traces with cividis colors\n",
    "        n_bold_regions = bold_sig.shape[1]\n",
    "        colors_anim_bold = cividis_cmap(np.linspace(0.2, 0.9, n_bold_regions))\n",
    "        for i in range(n_bold_regions):\n",
    "            ax1.plot(bold_sig[:, i], alpha=0.3, linewidth=0.8, color=colors_anim_bold[i])\n",
    "        ax1.set_xlabel('BOLD time point (TR)')\n",
    "        ax1.set_ylabel('BOLD signal')\n",
    "        ax1.set_title('BOLD Signal (all regions)')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_ylim(0, 2)\n",
    "\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        ax2.plot(raw_ts, alpha=0.5, linewidth=1, color=anim_blue)\n",
    "        ax2.axhline(target_fic, color=anim_gold, linestyle='--', linewidth=2)\n",
    "        ax2.set_xlabel('Time step')\n",
    "        ax2.set_ylabel('S_e')\n",
    "        ax2.set_title('Excitatory Activity (S_e)')\n",
    "        ax2.set_ylim(0, 1)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        # Row 1, Col 3: J_i distribution\n",
    "        ax3 = fig.add_subplot(gs[0, 2])\n",
    "        ax3.bar(range(n_nodes), J_i_vals, alpha=0.7, color=anim_mid)\n",
    "        ax3.set_xlabel('Region')\n",
    "        ax3.set_ylabel('J_i')\n",
    "        ax3.set_title('Inhibitory Weights (J_i)')\n",
    "        ax3.grid(True, alpha=0.3, axis='y')\n",
    "        ax3.set_ylim(0, 2.2)\n",
    "\n",
    "        # Row 2: FC matrices - use cividis for FC, diverging for difference\n",
    "        ax4 = fig.add_subplot(gs[1, 0])\n",
    "        im4 = ax4.imshow(fc_target, vmin=0, vmax=1.0, cmap='cividis')\n",
    "        ax4.set_title('Target FC')\n",
    "        plt.colorbar(im4, ax=ax4, fraction=0.046)\n",
    "\n",
    "        ax5 = fig.add_subplot(gs[1, 1])\n",
    "        im5 = ax5.imshow(fc_pred_mat, vmin=0, vmax=1.0, cmap='cividis')\n",
    "        ax5.set_title(f'Predicted FC r = {fc_corr_val:.2f}')\n",
    "        plt.colorbar(im5, ax=ax5, fraction=0.046)\n",
    "\n",
    "        ax6 = fig.add_subplot(gs[1, 2])\n",
    "        fc_diff = fc_pred_mat - fc_target\n",
    "        im6 = ax6.imshow(fc_diff, vmin=-0.5, vmax=0.5, cmap='RdBu_r')\n",
    "        ax6.set_title('FC Difference')\n",
    "        plt.colorbar(im6, ax=ax6, fraction=0.046)\n",
    "\n",
    "        # Row 3: wLRE and wFFI matrices - use cividis\n",
    "        ax7 = fig.add_subplot(gs[2, 0])\n",
    "        im7 = ax7.imshow(wLRE_mat, vmin=0.8, vmax=2.2, cmap='cividis')\n",
    "        wLRE_corr = np.corrcoef(wLRE_mat.flatten(), fc_target.flatten())[0, 1]\n",
    "        ax7.set_title(f'wLRE (r={wLRE_corr:.3f})')\n",
    "        plt.colorbar(im7, ax=ax7, fraction=0.046)\n",
    "\n",
    "        ax8 = fig.add_subplot(gs[2, 1])\n",
    "        im8 = ax8.imshow(wFFI_mat, vmin=0, vmax=1.2, cmap='cividis')\n",
    "        wFFI_corr = np.corrcoef(wFFI_mat.flatten(), fc_target.flatten())[0, 1]\n",
    "        ax8.set_title(f'wFFI (r={wFFI_corr:.3f})')\n",
    "        plt.colorbar(im8, ax=ax8, fraction=0.046)\n",
    "\n",
    "        # Row 3, Col 3: Convergence trajectory with position marker\n",
    "        ax9 = fig.add_subplot(gs[2, 2])\n",
    "        ax9_twin = ax9.twinx()\n",
    "\n",
    "        # Plot full trajectories with harmonized colors\n",
    "        line1 = ax9.plot(fc_correlations, linewidth=1.5, alpha=0.7, color=anim_blue, label='FC Correlation')\n",
    "        line2 = ax9_twin.plot(fc_rmse_values, linewidth=1.5, alpha=0.7, color=anim_gold, label='FC RMSE')\n",
    "\n",
    "        # Add vertical line at current position\n",
    "        current_iter = snapshots['iterations'][snapshot_idx]\n",
    "        ax9.axvline(current_iter, color=anim_mid, linestyle='--', linewidth=2, alpha=0.8)\n",
    "\n",
    "        # Add marker at current position\n",
    "        ax9.plot(current_iter, fc_correlations[current_iter], 'o', color=anim_blue, markersize=8, zorder=5)\n",
    "        ax9_twin.plot(current_iter, fc_rmse_values[current_iter], 'o', color=anim_gold, markersize=8, zorder=5)\n",
    "\n",
    "        # Labels and styling\n",
    "        ax9.set_xlabel('Iteration')\n",
    "        ax9.set_ylabel('FC Correlation', color=anim_blue)\n",
    "        ax9.tick_params(axis='y', labelcolor=anim_blue)\n",
    "        ax9_twin.set_ylabel('FC RMSE', color=anim_gold)\n",
    "        ax9_twin.tick_params(axis='y', labelcolor=anim_gold)\n",
    "        ax9.set_title('Convergence Trajectory')\n",
    "        ax9.grid(True, alpha=0.3)\n",
    "\n",
    "        # Legend\n",
    "        lines = line1 + line2\n",
    "        labels = [l.get_label() for l in lines]\n",
    "        ax9.legend(lines, labels, loc='center left', fontsize=8)\n",
    "\n",
    "        fig.suptitle(f'Iteration {iteration}/{n_eib_steps}', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Create animation with last frame pause\n",
    "    # Create frame sequence with last frame repeated to make it pause longer\n",
    "    n_frames = len(snapshots['iterations'])\n",
    "    last_frame_repeats = 5  # Repeat last frame 5 times (2.5 seconds at 2 fps)\n",
    "    frame_sequence = list(range(n_frames)) + [n_frames - 1] * last_frame_repeats\n",
    "\n",
    "    anim = FuncAnimation(\n",
    "        fig,\n",
    "        update_frame,\n",
    "        frames=frame_sequence,\n",
    "        interval=1000/fps,  # milliseconds between frames\n",
    "        repeat=True\n",
    "    )\n",
    "\n",
    "    # Save as GIF\n",
    "    writer = PillowWriter(fps=fps)\n",
    "    anim.save(output_path, writer=writer)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Animation saved to {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "# Create the GIF (may take a minute depending on number of snapshots)\n",
    "gif_path = create_animation_gif('ei_tuning_animation.gif', fps=2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![**Combined FIC+EIB tuning animation.** Evolution of neural dynamics and coupling parameters over iterations. Top row: BOLD signal for all regions, excitatory activity (S_e) converging to target (gold line), and inhibitory weights (J_i). Middle row: Target FC, predicted FC with quality metrics, and FC error (diverging colormap). Bottom row: Optimized coupling weights (wLRE, wFFI) shown in cividis, and dual-axis convergence trajectories (blue: correlation, gold: RMSE). All colors harmonized with cividis palette.](ei_tuning_animation.gif)\n",
    "\n",
    "### EIB Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# Setup evaluation model\n",
    "setup_eval_model()\n",
    "\n",
    "# Compute FC before EIB (but after initial FIC from earlier)\n",
    "print(\"Computing pre-EIB functional connectivity...\")\n",
    "fc_pre_eib = eval_fc(\n",
    "    state.dynamics.J_i,\n",
    "    state.coupling.coupling.wLRE,\n",
    "    state.coupling.coupling.wFFI\n",
    ")\n",
    "\n",
    "# Compute FC after combined FIC+EIB tuning\n",
    "print(\"Computing post-EIB functional connectivity...\")\n",
    "fc_post_eib = eval_fc(\n",
    "    state_ei.dynamics.J_i,\n",
    "    state_ei.coupling.coupling.wLRE,\n",
    "    state_ei.coupling.coupling.wFFI\n",
    ")\n",
    "\n",
    "# Compute quality metrics\n",
    "fc_corr_pre = fc_corr(fc_pre_eib, fc_target)\n",
    "fc_corr_post = fc_corr(fc_post_eib, fc_target)\n",
    "fc_rmse_pre = jnp.sqrt(jnp.mean((fc_pre_eib - fc_target)**2))\n",
    "fc_rmse_post = jnp.sqrt(jnp.mean((fc_post_eib - fc_target)**2))\n",
    "\n",
    "print(f\"\\nFC Quality Metrics:\")\n",
    "print(f\"  Pre-EIB  - Correlation: {fc_corr_pre:.4f}, RMSE: {fc_rmse_pre:.4f}\")\n",
    "print(f\"  Post-EIB - Correlation: {fc_corr_post:.4f}, RMSE: {fc_rmse_post:.4f}\")\n",
    "print(f\"  Improvement: Δcorr = {fc_corr_post - fc_corr_pre:+.4f}, ΔRMSE = {fc_rmse_post - fc_rmse_pre:+.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "fig = plt.figure(figsize=(8.1, 4.63))\n",
    "gs = fig.add_gridspec(2, 3, hspace=0.35, wspace=0.4, height_ratios=[1, 0.6])\n",
    "\n",
    "# Top row: FC matrices - use cividis\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "im1 = ax1.imshow(fc_target, vmin=0, vmax=1.0, cmap='cividis')\n",
    "ax1.set_title('Target FC\\n(Empirical)')\n",
    "ax1.set_xlabel('Region')\n",
    "ax1.set_ylabel('Region')\n",
    "plt.colorbar(im1, ax=ax1, fraction=0.046)\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "im2 = ax2.imshow(fc_pre_eib, vmin=0, vmax=1.0, cmap='cividis')\n",
    "ax2.set_title(f'Pre-EIB FC\\nCorr: {fc_corr_pre:.3f}, RMSE: {fc_rmse_pre:.3f}')\n",
    "ax2.set_xlabel('Region')\n",
    "plt.colorbar(im2, ax=ax2, fraction=0.046)\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "im3 = ax3.imshow(fc_post_eib, vmin=0, vmax=1.0, cmap='cividis')\n",
    "ax3.set_title(f'Post-EIB FC\\nCorr: {fc_corr_post:.3f}, RMSE: {fc_rmse_post:.3f}')\n",
    "ax3.set_xlabel('Region')\n",
    "plt.colorbar(im3, ax=ax3, fraction=0.046)\n",
    "\n",
    "# Bottom row: Dual-axis convergence plot spanning entire width\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "ax4_twin = ax4.twinx()\n",
    "\n",
    "# Plot FC correlation on left axis\n",
    "line1 = ax4.plot(fc_correlations, linewidth=2.5, color=accent_blue, label='FC Correlation')\n",
    "ax4.set_xlabel('EIB iteration')\n",
    "ax4.set_ylabel('FC Correlation with Target', color=accent_blue)\n",
    "ax4.tick_params(axis='y', labelcolor=accent_blue)\n",
    "ax4.set_xlim(0, len(fc_correlations))\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot FC RMSE on right axis\n",
    "line2 = ax4_twin.plot(fc_rmse_values, linewidth=2.5, color=accent_gold, label='FC RMSE')\n",
    "ax4_twin.set_ylabel('FC RMSE', color=accent_gold)\n",
    "ax4_twin.tick_params(axis='y', labelcolor=accent_gold)\n",
    "\n",
    "# Combined legend\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax4.legend(lines, labels, loc='center left')\n",
    "ax4.set_title('EIB Convergence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "This iterative implementation prioritizes clarity over performance. Python loops prevent JIT compilation, and manual state management adds overhead. Part 3 demonstrates a gradient-based alternative that achieves similar results with automatic differentiation.\n",
    ":::\n",
    "\n",
    "\n",
    "## Part 3: Gradient-Based Optimization Approach\n",
    "\n",
    "Rather than manually designing update rules, we reformulate EI tuning as a differentiable optimization problem: define a loss function combining FC error and activity deviation, mark parameters as optimizable, and use JAX autodiff with modern optimizers. This provides automatic gradients, adaptive learning rates, and cleaner code.\n",
    "\n",
    "The loss function combines FC RMSE with activity deviation from the FIC target:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# Prepare simulation\n",
    "t1_opt = 5 * 60_000\n",
    "dt_opt = 4.0\n",
    "solver_opt = BoundedSolver(Heun(), low=0.0, high=1.0)\n",
    "model_opt, state_opt = prepare(network, solver_opt, t1=t1_opt, dt=dt_opt)\n",
    "\n",
    "# Create BOLD monitor\n",
    "bold_monitor_opt = Bold(\n",
    "    period=bold_TR,\n",
    "    downsample_period=4.0,\n",
    "    voi=0,\n",
    "    history=result_init\n",
    ")\n",
    "\n",
    "print(\"Optimization model prepared\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "def loss(state):\n",
    "    \"\"\"Combined loss function for FC matching and E-I balance\"\"\"\n",
    "    # Simulate neural dynamics\n",
    "    ts = model_opt(state)\n",
    "\n",
    "    # Compute BOLD signal from simulated activity\n",
    "    bold = bold_monitor_opt(ts)\n",
    "\n",
    "    # Loss component 1: FC discrepancy with empirical data\n",
    "    fc_pred = compute_fc(bold, skip_t=30)  # Skip initial transient\n",
    "    fc_loss = rmse(fc_pred, fc_target)\n",
    "\n",
    "    # Loss component 2: Feedback inhibition control\n",
    "    # Penalize deviation from target excitatory activity level\n",
    "    mean_activity = jnp.mean(ts.data[-500:, 0, :], axis=0)  # Mean S_e over final timesteps\n",
    "    activity_loss = jnp.mean((mean_activity - target_fic) ** 2)\n",
    "\n",
    "    # Combined loss (both terms have similar scales)\n",
    "    return fc_loss + activity_loss\n",
    "\n",
    "# Evaluate initial loss\n",
    "initial_loss = loss(state_opt)\n",
    "print(f\"Initial loss: {initial_loss:.4f}\")\n",
    "\n",
    "# Mark parameters for optimization (J_i, wLRE, wFFI) with appropriate constraints\n",
    "state_opt.dynamics.J_i = Parameter(state_opt.dynamics.J_i)\n",
    "state_opt.coupling.coupling.wLRE = BoundedParameter(jnp.ones((n_nodes, n_nodes)), low=0.0, high=jnp.inf)\n",
    "state_opt.coupling.coupling.wFFI = BoundedParameter(jnp.ones((n_nodes, n_nodes)), low=0.0, high=jnp.inf)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "@cache(\"gradient_optimization\", redo=False)\n",
    "def run_gradient_optimization():\n",
    "    \"\"\"Run gradient-based optimization with caching.\"\"\"\n",
    "\n",
    "    # Create optimizer with AdaBelief algorithm\n",
    "    optimizer = OptaxOptimizer(\n",
    "        loss,\n",
    "        optax.adamaxw(learning_rate=0.033),\n",
    "        callback=MultiCallback([DefaultPrintCallback(), SavingLossCallback()])\n",
    "    )\n",
    "\n",
    "    # Run optimization for 50 steps\n",
    "    opt_state, opt_fitting_data = optimizer.run(state_opt, max_steps=66)\n",
    "\n",
    "    return opt_state, opt_fitting_data\n",
    "\n",
    "# Run optimization (cached)\n",
    "optimized_state, fitting_data = run_gradient_optimization()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# Evaluate optimized FC\n",
    "print(\"Evaluating optimized functional connectivity...\")\n",
    "fc_opt = eval_fc(\n",
    "    optimized_state.dynamics.J_i,\n",
    "    optimized_state.coupling.coupling.wLRE,\n",
    "    optimized_state.coupling.coupling.wFFI\n",
    ")\n",
    "\n",
    "fc_corr_opt = fc_corr(fc_opt, fc_target)\n",
    "fc_rmse_opt = rmse(fc_opt, fc_target)\n",
    "\n",
    "print(f\"\\nOptimization Results:\")\n",
    "print(f\"  Pre-Optimization  - Correlation: {fc_corr_pre:.4f}, RMSE: {fc_rmse_pre:.4f}\")\n",
    "print(f\"  Post-Optimization - Correlation: {fc_corr_opt:.4f}, RMSE: {fc_rmse_opt:.4f}\")\n",
    "print(f\"  Improvement: Δcorr = {fc_corr_opt - fc_corr_pre:+.4f}, ΔRMSE = {fc_rmse_opt - fc_rmse_pre:+.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# Extract loss values\n",
    "loss_values = fitting_data[\"loss\"].save\n",
    "n_steps = len(loss_values)\n",
    "\n",
    "fig = plt.figure(figsize=(8.1, 6))\n",
    "gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.4)\n",
    "\n",
    "# Top left: Loss trajectory - use cividis-derived colors\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.plot(loss_values, linewidth=2, color=\"k\", alpha=0.9)\n",
    "ax1.scatter(0, loss_values[0], s=80, color=accent_blue, zorder=5)\n",
    "ax1.scatter(n_steps-1, loss_values.array[-1], s=80, color=accent_gold, zorder=5)\n",
    "ax1.set_xlabel('Optimization Step')\n",
    "ax1.set_ylabel('Combined Loss')\n",
    "ax1.set_title('Loss Convergence')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Top middle: wFFI matrix - use cividis\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "im2 = ax2.imshow(optimized_state.coupling.coupling.wFFI, vmin=0, vmax=2, cmap='cividis')\n",
    "ax2.set_title('Optimized wFFI')\n",
    "ax2.set_xlabel('Source')\n",
    "ax2.set_ylabel('Target')\n",
    "plt.colorbar(im2, ax=ax2, fraction=0.046)\n",
    "\n",
    "# Top right: wLRE matrix - use cividis\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "im3 = ax3.imshow(optimized_state.coupling.coupling.wLRE, vmin=0, vmax=2, cmap='cividis')\n",
    "ax3.set_title('Optimized wLRE')\n",
    "ax3.set_xlabel('Source')\n",
    "ax3.set_ylabel('Target')\n",
    "plt.colorbar(im3, ax=ax3, fraction=0.046)\n",
    "\n",
    "# Bottom row: FC comparison - use cividis\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "im4 = ax4.imshow(fc_target, vmin=0, vmax=1.0, cmap='cividis')\n",
    "ax4.set_title('Target FC')\n",
    "plt.colorbar(im4, ax=ax4, fraction=0.046)\n",
    "\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "im5 = ax5.imshow(fc_pre_eib, vmin=0, vmax=1.0, cmap='cividis')\n",
    "ax5.set_title(f'Pre-Opt FC\\nCorr: {fc_corr_pre:.3f}')\n",
    "plt.colorbar(im5, ax=ax5, fraction=0.046)\n",
    "\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "im6 = ax6.imshow(fc_opt, vmin=0, vmax=1.0, cmap='cividis')\n",
    "ax6.set_title(f'Post-Opt FC\\nCorr: {fc_corr_opt:.3f}')\n",
    "plt.colorbar(im6, ax=ax6, fraction=0.046)\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)",
   "path": "/home/marius/Documents/Projekte/tvboptim/.venv/share/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}